{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbK5RKjaHvqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c883bb-3997-4a1a-db21-074712574128"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name())\n",
        "else:\n",
        "    print(\"CUDA is not available\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pn0bTVA5-RD",
        "outputId": "a1cd74a7-c7ac-4940-84fd-6a10b0c80f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Self\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Literal, Union\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "class LoRA_Base_Layer():\n",
        "\n",
        "    def __init__(self,\n",
        "                 rank=8,\n",
        "                 lora_alpha=8,\n",
        "                 lora_dropout=0.0,\n",
        "                 use_rslora=True):\n",
        "        self.rank=rank\n",
        "        self.lora_alpha= lora_alpha\n",
        "        self.lora_dropout=nn.Dropout(lora_dropout) if lora_dropout>0 else lambda x:x\n",
        "        self.use_rslora=use_rslora\n",
        "\n",
        "        self_scaling=self.lora_alpha/self.rank**0.5 if use_rslora else self.lora_alpha/self.rank\n",
        "\n",
        "    def _load_pretrained_weights(self, state_dict):\n",
        "        self.weight.data = state_dict[\"weight\"]\n",
        "        if \"bias\" in state_dict.keys():\n",
        "            self.bias.data = state_dict[\"bias\"]\n",
        "\n",
        "class LoRA_Linear_Layer(nn.Linear, LoRA_Base_Layer):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 out_features,\n",
        "                 bias=0, rank=8,\n",
        "                 lora_alpha=8,\n",
        "                 lora_dropout=0.0,\n",
        "                 use_rslora=True,\n",
        "                 **kwargs):\n",
        "      nn.Linear.__init__(self,\n",
        "                         in_features,\n",
        "                         out_features,\n",
        "                         bias,\n",
        "                         **kwargs)\n",
        "      LoRA_Base_Layer.__init__(self, rank=rank,\n",
        "                               lora_alpha=lora_alpha,\n",
        "                               lora_dropout=lora_dropout,\n",
        "                               use_rslora=use_rslora)\n",
        "      self.weight.requires_grad = False\n",
        "\n",
        "      self.LoRA_A=nn.Parameter(torch.zeros(in_features, rank))\n",
        "      self.LoRA_B=nn.Parameter(torch.zeros(rank, out_features))\n",
        "\n",
        "      nn.init.kaiming_uniform_(self.LoRA_A, a=math.sqrt(5))\n",
        "\n",
        "    def merged_weights(self):\n",
        "      merged_weights=self.weight.data + self.scaling * (self.LoRA_A @ self.LoRA_B)\n",
        "      state_dict = {\"weight\":self.weight, \"bias\":self.bias}\n",
        "      if self.bias is not None:\n",
        "        state_dict[\"bias\"]=self.bias\n",
        "\n",
        "      merged_linear=nn.Linear(self.in_features,\n",
        "                              self.out_features,\n",
        "                              bias=True if self.bias is not None else False)\n",
        "\n",
        "      merged_linear.load_state_dict(state_dict)\n",
        "\n",
        "      return merged_linear\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "      original_layer_out = F.linear(x, self.weight, bias=self.bias)\n",
        "      LoRA_Multiplication = (self.LoRA_A @ self.LoRA_B)* self.scaling\n",
        "      LoRA_rank_out=self.lora_dropout(x) @ LoRA_Multiplication\n",
        "      return original_layer_out + LoRA_rank_out\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  layer=LoRA_Linear_Layer(16,32, rank=2)\n",
        "  rand=torch.rand(4,16)\n",
        "\n",
        "  print(rand)\n"
      ],
      "metadata": {
        "id": "4xgUDIeo6ZCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a89fab3-b808-4915-842f-2375ca7be7de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.2750,  0.3382,  0.4056, -0.2825,  0.4470],\n",
            "        [ 0.2311,  0.1802,  0.2980,  0.2500,  0.3069],\n",
            "        [-0.3084, -0.1089, -0.4079, -0.2216,  0.1440],\n",
            "        [-0.1434, -0.2686,  0.2684, -0.1192,  0.4258],\n",
            "        [-0.0921, -0.0599, -0.2717, -0.2048, -0.1918]], requires_grad=True)\n",
            "None\n",
            "LoRA_Linear_Layer(in_features=5, out_features=5, bias=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4V3peqKRWR-m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}